# -*- coding: utf-8 -*-
"""Employee Dataset Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fblc6WgtDzuvOmk7epbTGOypaQDpsuhx
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn import linear_model
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler

from google.colab import files
data = files.upload()

data = pd.read_csv('Dummy_5000_Employee_Details_Dataset (1).csv')

"""# Preprocessing"""

data.info()

data.duplicated().sum()

data.isnull().sum()

# Fill missing values for string features with the most frequent value
string_features = data.select_dtypes(include=['object']).columns
for feature in string_features:
  data[feature] = data[feature].fillna(data[feature].mode()[0])

# Fill missing values for integer features with the median value
int_features = data.select_dtypes(include=['int64', 'float64']).columns
for feature in int_features:
  data[feature] = data[feature].fillna(data[feature].median())

data.isnull().sum()

data.to_csv('employee_dataset_clean.csv')
files.download('employee_dataset_clean.csv')

# Convert 'DOJ' and 'DOB' to datetime
data['DOJ'] = pd.to_datetime(data['DOJ'])
data['YearDOJ'] = data['DOJ'].dt.year
data['DOB'] = pd.to_datetime(data['DOB'])
data['YearDOB'] = data['DOB'].dt.year

data2 = data.copy()

# Encode the categorical columns
le_Insurance = LabelEncoder()
le_Marital_Status = LabelEncoder()
le_Department  = LabelEncoder()
le_Position  = LabelEncoder()
le_Sex = LabelEncoder()

data2['Sex1'] = le_Sex.fit_transform(data2['Sex'])
data2['Insurance1'] = le_Insurance.fit_transform(data2['Insurance'])
data2['Marital Status1'] = le_Marital_Status.fit_transform(data2['Marital Status'])
data2['Department1'] = le_Department.fit_transform(data2['Department'])
data2['Position1'] = le_Position.fit_transform(data2['Position'])

data2 = data2.drop(['Name', 'Address', 'DOJ', 'DOB', 'Sex', 'Insurance', 'Marital Status', 'Department', 'Position'], axis=1)

"""# Data Information"""

data2.head()

correlation_matrix = data2.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

data2['Salary'].describe()

data2['Salary'].hist(bins=30)
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

"""# Data Prediction Model

## Linear Regression
"""

X = data2.drop(['Salary'], axis=1)
Y = data2['Salary']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

model = linear_model.LinearRegression()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

mse = mean_squared_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print("cross-validation:", model.score(X_test, Y_pred))

"""## Random Forest"""

rand = RandomForestRegressor(n_estimators=100, random_state=0, oob_score=True)

rand.fit(X_train, Y_train)

pred = rand.predict(X_test)

mse = mean_squared_error(Y_test, pred)
r2 = r2_score(Y_test, pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print("cross-validation:", cross_val_score(rand, X_test, pred, cv=3).mean())

"""## Decision Tree"""

dec = DecisionTreeRegressor(random_state=0)

dec.fit(X_train, Y_train)

y_pred = dec.predict(X_test)

mse = mean_squared_error(Y_test, y_pred)
r2 = r2_score(Y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print("cross-validation:", cross_val_score(dec, X_test, pred, cv=3).mean())

"""## XGBoost"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

xgboost = xgb.XGBRegressor()

grid_search = GridSearchCV(xgboost, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train_scaled, Y_train)

print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validation score: ", grid_search.best_score_)

best_model = grid_search.best_estimator_

Ypred = best_model.predict(X_test_scaled)

mse = mean_squared_error(Y_test, Ypred)
r2 = r2_score(Y_test, Ypred)
scores = cross_val_score(best_model, X_test_scaled, Ypred, cv=3).mean()

print("Mean Squared Error:", mse)
print("R-squared:", r2)
print("Cross-validation scores: ", scores)